{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wJpXpmjEYC_T"
   },
   "source": [
    "## Building a hexa GPT\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "h5hjCcLDr2WC",
    "outputId": "45566b47-231b-4a88-9ee0-79eff93319ae"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['https://raw.githubusercontent.com//itrewub/jh-notes/main/All/10%E5%B2%81%E7%94%B7%E5%AD%A9%E7%BB%8F%E5%B8%B8%E6%83%85%E7%BB%AA%E5%A4%B1%E6%8E%A7%E6%98%AF%E5%8F%97%E7%88%B8%E7%88%B8%E8%84%BE%E6%B0%94%E7%9A%84%E5%BD%B1%E5%93%8D%E8%BF%98%E6%98%AF%E7%8E%AF%E5%A2%83%E5%BD%B1%E5%93%8D%E5%AF%BC%E8%87%B4%E7%9A%84%E5%BA%94%E8%AF%A5%E5%A6%82%E4%BD%95%E6%AD%A3%E7%A1%AE%E5%9C%B0%E5%BC%95%E5%AF%BC-x10958066763y1043771095.md', 'https://raw.githubusercontent.com//itrewub/jh-notes/main/All/12-%E6%97%A5%E8%B4%B5%E5%B7%9E%E5%AE%89%E9%A1%BA%E5%A4%A7%E5%B7%B4%E8%BD%A6%E5%9D%A0%E6%B9%96%E4%BA%8B%E4%BB%B6%E9%80%9A%E6%8A%A5%E7%B3%BB%E5%8F%B8%E6%9C%BA%E5%9B%A0%E5%AF%B9%E6%8B%86%E8%BF%81%E4%B8%8D%E6%BB%A1%E8%93%84%E6%84%8F%E6%8A%A5%E5%A4%8D%E7%A4%BE%E4%BC%9A%E8%BF%98%E6%9C%89%E5%93%AA%E4%BA%9B%E6%96%B0%E7%9A%84%E4%BF%A1%E6%81%AF%E9%87%8F-x10621254739y1011320797.md']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import requests\n",
    "\n",
    "def get_raw_links(github_url):\n",
    "    # 获取GitHub页面的内容\n",
    "    response = requests.get(github_url)\n",
    "\n",
    "    # 从页面内容中提取出文件名\n",
    "    file_names = re.findall(r'href=\"/.*?\\.md\"', response.text)\n",
    "    file_names = [fn[6:-1] for fn in file_names]\n",
    "\n",
    "    # 获取每个文件的raw格式链接\n",
    "    raw_links = []\n",
    "    for file_name in file_names:\n",
    "        file_name = file_name.replace(\"/blob\", \"\")\n",
    "        raw_link = f\"https://raw.githubusercontent.com/{file_name}\"\n",
    "        raw_links.append(raw_link)\n",
    "\n",
    "    # 返回所有的raw格式链接\n",
    "    return raw_links\n",
    "\n",
    "ans_links = get_raw_links('https://github.com/itrewub/jh-notes/tree/main/All')\n",
    "print(ans_links[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QZgOoXJ_72L1",
    "outputId": "0341619b-6134-4a58-c31c-e5c9cbfe7709",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#print(ans_links)\n",
    "#https://raw.githubusercontent.com/itrewub/jh-notes/main/All/12-%E6%97%A5%E8%B4%B5%E5%B7%9E%E5%AE%89%E9%A1%BA%E5%A4%A7%E5%B7%B4%E8%BD%A6%E5%9D%A0%E6%B9%96%E4%BA%8B%E4%BB%B6%E9%80%9A%E6%8A%A5%E7%B3%BB%E5%8F%B8%E6%9C%BA%E5%9B%A0%E5%AF%B9%E6%8B%86%E8%BF%81%E4%B8%8D%E6%BB%A1%E8%93%84%E6%84%8F%E6%8A%A5%E5%A4%8D%E7%A4%BE%E4%BC%9A%E8%BF%98%E6%9C%89%E5%93%AA%E4%BA%9B%E6%96%B0%E7%9A%84%E4%BF%A1%E6%81%AF%E9%87%8F-x10621254739y1011320797.md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JV7fI3ia24RX",
    "outputId": "9857e00d-c76b-434a-b266-85ff6de964cd",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10岁男孩经常情绪失控，是受爸爸脾气的影响还是环境影响导致的？应该如何正确地引导？\n",
      "Tags: Clan\n",
      "\n",
      "10岁男孩经常情绪失控，是受爸爸脾气的影响还是环境影响导致的？应该如何正确地引导？\n",
      "\n",
      "让我们来梳理一下你的问题。\n",
      "首先，儿子的表现令你感到忧虑和失望，这导致了痛苦。\n",
      "为这个痛苦，你希望定位原因。\n",
      "你定位原因的时候遇到了这样一种纠结——如果定位在自己身上，不知道该怎么总结自己哪里有错。某种程度上说，你现在其实已经尽了全力了——有哪位父母不是已经尽了全力了呢？所以，就算定位在自己身上，也无余力可用了，定位在自己身上又有何意义呢？\n",
      "而无论是定位在父亲身上、祖父母身上、还是环境身上，都一样沉重。定位到ta们身上，要怎么改变ta们呢？离婚吗？还是威逼利诱、说服谈判要ta们改变呢？难道现在不也已经是尽了全力管控了这些你觉得有害的因素，实在只能管控到这个程度，没有办法了吗？\n",
      "其实，你真正的困境并不是看到有各种有害的因素存在，没有办法去解决，而是就算有办法，你担忧没有力气解决。\n",
      "其实，没有力气为子女争取自己理想中最低限度的生活了，这种绝望感，才是真正的问题。\n",
      "你累了，这才是真正的问题。\n",
      "你现在处在想要问到一个好的办法，然后逼着自己再挤出一份力量来再搏一搏的状态。\n",
      "你跟子女讲话，看似心平气和，温言细语，但其实是竭尽全力的控制着才压住的，不是吗？\n",
      "你不希望自己的焦虑影响到子女，想要ta们不要平添这份负担，所以在控制着，不是吗？\n",
      "光是这份克制，已经让人精疲力竭了，不是吗？\n",
      "在这份重压之下，再给你更多的妙计，其实对你是没什么用的。因为你其实没有力气再负担更多了。勉强扛更多的责任，你也很快会无以为继。因为无以为继，这个新的拯救计划坦白说不太可能被有效实施，而半途而废的努力，只会让你陷入更深的忧虑。\n",
      "这个恶性循环迟早要教会你一件事——此路不通。\n",
      "你必须有某种方法，先把心头的重担卸下。先让自己有心力可以拿来做改善才行。不先卸掉这副担子，任何妙计都没有燃料，也难有好结果。\n",
      "而更进一步的，真的把这担子卸掉了，你的精气神直接就会有一些良性的改变。同样的话，用不同的精神状态去说，效力是截然不同的。有很大的概率，那个卸下了重担的你，没有改变任何原有的策略和节奏，就已经把事情引向了别的方向。\n",
      "而这个千斤重担是什么呢？\n",
      "就是“把子女‘不正常’的人生轨迹趋势，看作一件绝对要想办法避免的坏事，并且把避免不了它看成一种绝对失职和失败”。\n",
      "真正在教育人、决定人的命运的，不是父母，而是世界。父母为子女设计命运，无论父母们自己自认为多么的保守、打了多少余量，都从一开始是受诅咒的。\n",
      "你自认为要得不多，要的只是你觉得绝大多数人都自然拥有的最基本的东西，所以你觉得你有权、有立场、也应该有机会这样要求。这本身就是一种傲慢，而且是一种根本性的错误。\n",
      "因为这些安排，这些你觉得万分“不公”、万分“不好”的安排，是否最后会孵化为坏的结局、并不是确定的，明白吗？\n",
      "少年时遇到的极大的困境，乃至于折磨、伤害和痛苦，真正决定的，并不是子女必定会拥有失败的、可悲的一生，而是必将拥有非常规的一生。\n",
      "你们看到逻辑断裂的地方了吗？父母们？\n",
      "看到没有？\n",
      "“非常规”，还包括了伟大啊。\n",
      "你们难道没有发现，那些青史留名、照耀人类文明的名字，常常有着超乎常人的成长历程吗？\n",
      "ta们为什么能看见别人看不见的痛苦，追寻别人无法坚持的追求，坚持到常人坚持不到的程度，踏入常人无法踏入的境界？\n",
      "当你们竭尽全力仍然不能阻止的某种痛苦的命运降临到你们的子女身上，那意味着什么？\n",
      "你们的孩子，被选中了。\n",
      "明白吗？\n",
      "这种天意的选择，是会发育成憎恨世界的恶魔，还是会发育成将来拯救人类的天使，极大程度上取决于你们自己对待这被选本身的态度。\n",
      "你们视其为诅咒，无所不用其极的、乃至于超越正义、伦理边界的谋求逃避，在逃避失败之后怨毒、罥骂、诅咒，那么你们的那份焦虑、沮丧、怨恨和无能就会一滴一滴的、一秒一秒的浸透子女的命运。\n",
      "你们视其为blessing in disguise，你们相信这些苦难将来必将孕育伟大的灵魂，你们就不会惊慌失措，你们的应对就能发乎于情、止乎于理，能尽最大可能改变可以改变的，而能坦然接受应该接受的。你们不会为了逃避痛苦而疯狂，你们就在子女面前亲自折断了痛苦的毒针。\n",
      "痛苦？那就痛苦呗？痛苦并不需要全部避免。\n",
      "痛苦对人的真正的伤害，是靠造成不择手段的逃避，引诱人连环犯错，陷入自己无法面对的罪恶、更无法面对的痛苦威胁，一路狂奔。是那些二次、三次、四次在试图逃避中做出的更大的错误决定，才是痛苦最大的杀伤所在。\n",
      "你们其实是子女们面对痛苦和艰难的第一个老师，而你们该教的第一课，不是如何巧妙的不要落入这种境地，也不是如何努力的挣脱这种境地，而是从容、淡定的接受它，相信它只是一份处理工艺复杂、兑换过程漫长的财富的信心。\n",
      "那会永远的让ta们免疫于一切灾难和困境。\n",
      "面对苦难、困难和失败，没有办法避免、也没有办法解决，但也并不因此抑郁、惊慌、绝望，仍然能对他人尽义、对未来抱有极大的乐观，甚至相信这是一份未来的赐福，这其实是父母们唯一真正需要做的教育。\n",
      "而且这份教育，只需要一个人就能完成。甚至其他人的不配合、其他人的反其道而行，恰恰能形成反向对照组，让子女更清晰明确的看到面对苦难、失败、困难应该有的样子是什么。\n",
      "只要下定决心，没有任何其他因素能阻止，也没有任何事情能让你失败。\n",
      "我已经对你说了这话，你已经看见了这话，你已经不会失败了。\n",
      "你对ta有多少爱，你离失败就有多远。你的爱支撑你取胜，是绰绰有余的。\n",
      "你的子女遭遇了你觉得其他小孩都不会遭遇的事情，那仅仅意味着ta将来的命运必定会与一般小孩不再相同。\n",
      "你要反省一下，你们的逻辑推断为什么会跳过这一步？这本身就是一道伤痕。\n",
      "很多人跛行已久，已经觉得这是正常的了。\n",
      "不同寻常，是指“不同寻常的坏”，还是指“不同寻常的好”，首先取决于你们觉得ta遭遇的这件事本身是不同寻常的坏，还是不同寻常的好。\n",
      "当你们认定偏离你们的计划安排全是不同寻常的坏的时候，你们是在自居为全知、全能、全善的上帝，以至于任何你们不认可的安排，都是毫无疑问的、必须排除的恶。\n",
      "这个心态本身，决定了一切“妙计”必定不过是苟延残喘，勉强拖延，它一定会找到种种你们难以想象、更谈不上应对的逻辑路径，把“不寻常”推向恶。\n",
      "顺便，仔细想想，你的孩子对失败的不接受，真的只是从父亲身上学来的吗？\n",
      "重新读一遍你的问题，难道你的问题本身，不正是对失败的不接受吗？\n",
      "你又何尝不是“道理都懂，也都知道失败没什么大不了，但一遇到还是放不下”吗？\n",
      "他那样的反应，其实恐怕是大家联手教的，他其实可能就没有看到任何一个真正的可以坦然接受挫折的示范。\n",
      "他需要看到一个真正乐观的、运转良好的例子。\n",
      "这并不需要“一整个团队的全部队友互相配合来创造”——如果必须要这样的条件才能保持乐观，与其说是在示范乐观的可能，不如说是在示范乐观的不可能。\n",
      "如果乐观不可能，那么又输了一场比赛、又失去了一群人的尊敬，怎么能说是无关痛痒的小事呢？\n",
      "父母们，好好想想吧。\n",
      "其实你们的子女，谁没有面临与众不同的痛苦和困难？这个问题，和你想问的，其实只是细节不同而已吧？\n",
      "世上哪有“一般的命运”？谁真的没有被选中呢？\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import markdown\n",
    "from bs4 import BeautifulSoup\n",
    "def get_text(url):\n",
    "  response = requests.get(url)\n",
    "  md_content = response.text\n",
    "\n",
    "  html_content = markdown.markdown(md_content)\n",
    "  soup = BeautifulSoup(html_content, 'html.parser')\n",
    "  # 获取纯文本内容\n",
    "  str_content = soup.get_text()\n",
    "  return str_content\n",
    "\n",
    "print(get_text(ans_links[0]))\n",
    "ans = [get_text(url) for url in ans_links]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6xWI_VyAsN8F",
    "outputId": "ca0d716d-b26f-4ff8-a5a1-21624a551043"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34 岁华裔工程师在美国遭抢笔记本电脑，追击歹徒被拖行后身亡，为什么会发生此类事件？是个案吗？\n",
      "Tags: Society, Clan\n",
      "\n",
      "34 岁华裔工程师在美国遭抢笔记本电脑，追击歹徒被拖行后身亡，为什么会发生此类事件？是个案吗？\n",
      "\n",
      "美国是一个高度依赖自卫来保证安全的国家，不能沿用在东亚社会里养成的安全观。\n",
      "美国人对人的生命安全看得其实不重，并不像中国有一种“人命大过天”的观念。\n",
      "在美国，人们重视的是基于契约而产生的责任。他们并不是重视人命本身，而是重视常常因此而来的天价赔偿责任。\n",
      "身在美国，你要顺着契约链条一路追踪下去，会发现你经常处在一种没有任何人要为你的某种死亡负责的状态。\n",
      "这类问题，他们往往用自购保险加免责条款的方式处理了。先用免责条款请你自担风险，然后建议你购买保险以防万一。\n",
      "就这样了。\n",
      "如果要服务方为你的安全负责，那么这项服务一定是天价——坦白说就是把他要买的保险拆成你的一人份然后翻了几倍强制打包在服务费里卖给你了罢了。\n",
      "而即便如此，服务方仍然是受到有限责任公司制度保护的。说不定向你提供服务的企业根本就没有购买保险，就是打算一旦出事就破产了事罢了。\n",
      "这样的好处是，美国人做起生意来胆子很大——因为各种风险都有一个可以预见的保底。哪怕闹出人命也有一个确切的数字——甚至这个数字未必很高。\n",
      "坏处是美国在本质上就不安全，需要的安全意识、安全技能、安全投入都很大。\n"
     ]
    }
   ],
   "source": [
    "print(ans[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2c5V0FvqseE0",
    "outputId": "0a654812-2263-44bc-d886-e8cb70b7f890"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n"
     ]
    }
   ],
   "source": [
    "# let's look at the first 1000 characters\n",
    "print(len(ans_links))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0e-Rbyr8sfM8",
    "outputId": "3ccd1c27-8ff4-431d-fd53-5a3fb3b63e24"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\zhang\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 0.540 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4179\n"
     ]
    }
   ],
   "source": [
    "#!pip install jieba\n",
    "import jieba\n",
    "chars = set()\n",
    "\n",
    "for lst in ans:\n",
    "    \n",
    "    for item in lst:\n",
    "        words = jieba.cut(item)\n",
    "        \n",
    "        for word in words:\n",
    "           # print(word)\n",
    "            chars.add(word)\n",
    "vocab_size = len(chars)\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in chars:\n",
    "    if len(i) > 1:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Yw1LKNCgwjj1",
    "outputId": "cdf597de-a527-4450-e54f-1e82c8654d23"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1310, 90, 2318]\n",
      "我爱你\n"
     ]
    }
   ],
   "source": [
    "# create a mapping from characters to integers\n",
    "stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "itos = { i:ch for i,ch in enumerate(chars) }\n",
    "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
    "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
    "\n",
    "print(encode(\"我爱你\"))\n",
    "print(decode(encode(\"我爱你\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YJb0OXPwzvqg",
    "outputId": "f1e1c891-519f-4590-cbdb-ff7f744aed02"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2380, 4075, 4081, 1668, 3449,  219,  406, 2943, 4037, 1382,  104,  469,\n",
      "        3647, 1092, 2326, 2326, 4032, 1434, 3472, 2855, 1844, 1661, 3647,  228,\n",
      "         750, 2855, 1844,  451,  471, 3472, 3466, 2144, 2721, 4003, 1946, 1662,\n",
      "        1298,  312, 2833,  451, 3466,  224, 1951, 3614,  138, 3698, 1608, 1205,\n",
      "        3300, 2640, 3614, 2702,  224,  224, 2380, 4075, 4081, 1668, 3449,  219,\n",
      "         406, 2943, 4037, 1382,  104,  469, 3647, 1092, 2326, 2326, 4032, 1434,\n",
      "        3472, 2855, 1844, 1661, 3647,  228,  750, 2855, 1844,  451,  471, 3472,\n",
      "        3466, 2144, 2721, 4003, 1946, 1662, 1298,  312, 2833,  451, 3466,  224,\n",
      "         224, 2530, 1310, 1346, 3679,  233, 1544,  388, 3165, 2318, 3472, 2357,\n",
      "        2200, 2676,  224, 1113, 3088,  469, 3924, 3134, 3472, 3927,  342, 3390,\n",
      "        2318, 2228,  487, 1377, 3151, 3980, 1382, 3576,  469, 2145,  451,  471,\n",
      "        2927, 1293, 4061, 2676,  224,  913, 2145, 3495, 1293, 4061,  469, 2318,\n",
      "        3439, 3576, 3462, 1401, 1726, 2709, 2676,  224, 2318, 3462, 1401, 1726,\n",
      "        2709, 3472,  844, 1066, 1170,  487, 2927, 2145,  681,  388,  157, 2879,\n",
      "        1909,  306,  306, 4003, 1123, 3462, 1401, 3387, 2247, 2499, 4027,  162,\n",
      "         469, 1453,   34, 3752, 2721, 1987, 1250, 2576, 1909, 2247, 2499, 3452,\n",
      "        1475, 3124,  490, 2676, 1049,  157, 1677, 3453,  162, 3021,  469, 2318,\n",
      "         342, 3387,  535, 2180, 2848,  219, 3824, 2927,  464, 2139, 2927,  306,\n",
      "         306, 3124, 3452, 1401, 1100, 4042, 1453, 3647, 2848,  219, 3824, 2927,\n",
      "         464, 2139, 2927, 4000, 3466, 1004, 1970,  469, 2168, 1919, 3462, 1401,\n",
      "        3387, 2247, 2499, 4027,  162,  469, 2840, 3855, 2142, 2139, 1145, 3299,\n",
      "        2927,  469, 3462, 1401, 3387, 2247, 2499, 4027,  162,  578, 3124, 1946,\n",
      "        3328, 2411, 4000, 3466,  224, 1609, 3855, 1462, 3647, 3462, 1401, 3387,\n",
      "        1100, 3322, 4027,  162, 2045, 4109, 1100, 4042, 4027,  162, 2045, 1661,\n",
      "        3647,  228,  750, 4027,  162,  469, 3552,  388,  681,  529, 3314, 2676,\n",
      "        3462, 1401,  487, 1698, 3614, 1346, 4027,  162,  469, 1711, 1987, 1250,\n",
      "        2841,  489, 1698, 3614, 1346, 4000, 3466,  470, 2044, 1095, 3466, 1661,\n",
      "        3647, 4058, 3249, 3274, 3965, 2045, 3021, 3497, 1561,  123, 1711, 1698,\n",
      "        3614, 1346, 2841,  489, 4000, 3466, 2942, 3752,  342, 3387, 1453, 2840,\n",
      "        2848,  219, 3647, 3824, 2927,  464, 2139, 1511,  104, 2927, 2145, 1492,\n",
      "        2318, 3856, 3262, 3124, 2307, 3472, 2709, 1536,  469, 2180, 3387, 1160,\n",
      "        1616, 1511,  104,  487, 2145, 3495, 1677, 3453,  469, 2070, 3124, 1282,\n",
      "        2594, 2927, 1095, 3466,  224,  535, 2180,  469, 2318,  118, 1662, 3472,\n",
      "        3995,  750,  186, 1453, 3647,  369,  487, 3124,  690,  157, 3124, 2307,\n",
      "        3472, 2709, 1536, 4137, 3387,  469, 2070, 3124, 1282, 2594, 3653, 1779,\n",
      "         780,  469, 1609, 3647, 2168, 1919, 3124, 1282, 2594,  469, 2318,  632,\n",
      "        1377, 2070, 3124, 2139, 1434, 1779,  780, 2676,  224,  535, 2180,  469,\n",
      "        2070, 3124, 2139, 1434,  913, 3134, 3402, 1543, 2888, 2247, 2499, 1544,\n",
      "         370, 2568, 3979,  495,  504, 3453, 3472, 1175, 2520, 2927,  469, 2145,\n",
      "         157, 1631, 3576, 2228,  469, 3436, 3647,  118, 1662, 3472, 2357, 2200,\n",
      "        2676,  224, 2318, 1431, 2927,  469, 2145, 3436, 3647,  118, 1662, 3472,\n",
      "        2357, 2200, 2676,  224, 2318,  342, 3387, 3197, 3387,  370, 1711, 2357,\n",
      "         487,  388, 3495, 1481, 3472, 1282, 2594,  469,  142, 3020, 3249,  319,\n",
      "        2247, 2499, 1195, 3232, 2932,  388, 2953, 2139, 3791, 3679, 1195, 1564,\n",
      "         388, 1564, 3472, 2513, 3075, 2676,  224, 2318, 2014, 3134, 3402,   47,\n",
      "        1639,  469,  369,  712, 3049, 3048, 1434, 3980,  469, 2573, 3821, 2600,\n",
      "         728,  469,  725,  535, 2180, 3647, 1333, 3824,  464, 2139, 3472,  104,\n",
      "        2558,  319, 3436, 3352, 3678, 3472,  469, 1453, 3647, 1095, 3466,  224,\n",
      "        2318, 1453, 3439, 3576, 2247, 2499, 3472, 3304, 3151, 2855, 1844,  487,\n",
      "        3134, 3402,  469,  370, 1711, 1698, 3614, 1346, 1453, 1711, 3048, 3534,\n",
      "        2145, 2953,  652,  632,  469, 1004, 1970, 3387,  104, 2558,  319,  469,\n",
      "        1453, 3647, 1095, 3466,  224, 3138, 3647, 2145, 2953, 4100, 2558,  469,\n",
      "        2848,  219, 2530, 1442, 1345, 2390, 2139, 1333, 2927,  469, 1453, 3647,\n",
      "        1095, 3466,  224, 3387, 2145, 2953, 3314, 3352, 2401, 3165,  469, 1195,\n",
      "        2172, 2318, 2277, 1585, 3472,  547, 2171,  469,  535, 2180, 1144, 2318,\n",
      "        3647, 2070, 3247, 1250, 3299, 3472, 2676, 2709,  913, 2318,  535, 2180,\n",
      "        2070, 3124, 2139, 1434, 1195,  652,  632, 2277, 1585, 2927, 2676, 1876,\n",
      "        3398, 2864, 2277, 1585, 3472, 1408, 3080,  469, 2318, 2840, 1989, 2658,\n",
      "         622, 3855, 1970,  913,  488, 2676, 2709,  913, 3855, 1970,  913,  488,\n",
      "         469, 2145, 3495, 2743, 3472,  325,  145, 2171,  917,  647,  699, 3021,\n",
      "        1453, 1519, 1145, 1616,  928, 3124, 1283, 2180,   17,  469, 1609, 3282,\n",
      "        2188, 1609, 2118, 3472, 2579, 2139,  469, 1160,  622, 2530, 2318, 3399,\n",
      "        3191, 2277, 1907, 3472, 1377, 3151, 2676,  224, 2145, 3495, 2736, 2492,\n",
      "        1552,  228, 2651,  327, 1711, 3739,  622, 2318,  388, 3746, 2780,  306,\n",
      "         306,   68, 3618, 1453, 2246, 2676,  224, 2318, 3914, 3796, 3124, 1049,\n",
      "         157,   96, 2594,  469, 3088, 1235, 3049, 1860, 3472, 3314,  632,  721,\n",
      "        3165, 2676, 3088, 2530, 2247, 2499, 3124, 3049, 2139, 1145, 1970, 2585,\n",
      "        3679,  261, 2841, 1280, 3436, 1046, 2676, 1453, 3088,  721, 3207, 2145,\n",
      "        2968,  632, 3134,  469, 3080, 1946,  547, 2171, 3552, 2070, 3124,  635,\n",
      "         653,  469, 2840, 2942, 3124, 1481, 1909, 1123, 2676,  224, 1609, 2277,\n",
      "        4004,  388, 1403, 3472,  469,  118, 3472, 1235, 2145,  632, 3134,  721,\n",
      "        3207, 2927,  469, 2318, 3472, 1345, 1434, 2724, 1761, 3747, 2168,  622,\n",
      "        3124,  388, 1492,  337, 2492, 3472, 2841,  489, 2676,  862,  681, 3472,\n",
      "        1639,  469, 3299, 1453,  862, 3472, 1345, 2724, 2513, 3075, 3653, 3021,\n",
      "         469, 1283, 2139, 3647, 3476,  142, 1453,  862, 3472, 2676, 3124, 1989,\n",
      "         427, 3472, 2208,  741,  469, 2634, 3495,  721, 3165, 2927, 3314,  632,\n",
      "        3472, 2318,  469, 2070, 3124, 2841,  489, 3080, 1946, 1726, 3124, 3472,\n",
      "        2189,  346, 3980, 3433, 1500,  469, 2168, 2848,  219, 1235, 2780, 2943,\n",
      "        2833, 3650, 2927, 3754, 3472,   96, 3650, 2676,  224, 1609, 2145, 3495,\n",
      "        2293, 3680, 3314,  632, 3647, 3247, 1250, 4000, 3466,  224, 2168, 3647,\n",
      "        4062, 1235, 3134, 3402, 3801, 1453, 1662,  406, 2004, 3472, 1442, 1175,\n",
      "        2758,  700, 2830, 2722,  469,  369,  679,  388, 3746, 1631, 1144, 1711,\n",
      "         370, 1282, 2594, 2184,  353, 3472, 1456, 2780,  469,  186, 1137, 1235,\n",
      "        2184,  353, 1453, 2927])\n"
     ]
    }
   ],
   "source": [
    "# let's now encode the entire text dataset and store it into a torch.Tensor\n",
    "import torch # we use PyTorch: https://pytorch.org\n",
    "data = [torch.tensor(encode(text), dtype=torch.long) for text in ans]\n",
    "print(data[0][:1000]) # the 1000 characters we looked at earier will to the GPT look like this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "f_WIXqxz0lU5"
   },
   "outputs": [],
   "source": [
    "# Let's now split up the data into train and validation sets\n",
    "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "TD5Bj8Y6IAD4",
    "outputId": "2ce8fa38-a968-4935-d1c7-5541a31c08e6"
   },
   "outputs": [],
   "source": [
    "block_size = 10\n",
    "decode(train_data[0][:block_size+1].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9HXDe8vGJCEn",
    "outputId": "588663aa-1de5-4ef7-aba0-4a96fe828353"
   },
   "outputs": [],
   "source": [
    "# x = train_data[:block_size]\n",
    "# y = train_data[1:block_size+1]\n",
    "# for t in range(block_size):\n",
    "#     context = x[:t+1]\n",
    "#     target = y[t]\n",
    "#     print(f\"when input is {context} the target: {target}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Q3k1Czf7LuA9",
    "outputId": "f53872bd-bfc1-4d89-dd2e-9e9e68b783b9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs:\n",
      "torch.Size([3600, 12])\n",
      "tensor([[3472, 3747, 1092,  ..., 3647,  388, 2953],\n",
      "        [ 622,  913, 2927,  ..., 1108,  469, 2318],\n",
      "        [2530, 1442, 1345,  ..., 3647, 1095, 3466],\n",
      "        ...,\n",
      "        [3048,  205, 3472,  ..., 2224,  469, 2318],\n",
      "        [1946, 1144, 2145,  ..., 3848, 2397, 2045],\n",
      "        [1711,  913, 2247,  ..., 2676,  224, 2318]], device='cuda:0')\n",
      "targets:\n",
      "torch.Size([3600, 12])\n",
      "tensor([[3747, 1092, 1945,  ...,  388, 2953, 3197],\n",
      "        [ 913, 2927, 3078,  ...,  469, 2318, 1346],\n",
      "        [1442, 1345, 2390,  ..., 1095, 3466,  224],\n",
      "        ...,\n",
      "        [ 205, 3472, 2888,  ...,  469, 2318, 2247],\n",
      "        [1144, 2145,  402,  ..., 2397, 2045, 3856],\n",
      "        [ 913, 2247, 2499,  ...,  224, 2318, 4003]], device='cuda:0')\n",
      "----\n",
      "when input is 的 the target: 3747\n",
      "when input is 的接 the target: 1092\n",
      "when input is 的接受 the target: 1945\n",
      "when input is 的接受它 the target: 469\n",
      "when input is 的接受它， the target: 555\n",
      "when input is 的接受它，相 the target: 1349\n",
      "when input is 的接受它，相信 the target: 1945\n",
      "when input is 的接受它，相信它 the target: 1160\n",
      "when input is 的接受它，相信它只 the target: 3647\n",
      "when input is 的接受它，相信它只是 the target: 388\n",
      "when input is 的接受它，相信它只是一 the target: 2953\n",
      "when input is 的接受它，相信它只是一份 the target: 3197\n",
      "when input is 会 the target: 913\n",
      "when input is 会为 the target: 2927\n",
      "when input is 会为了 the target: 3078\n",
      "when input is 会为了逃 the target: 2184\n",
      "when input is 会为了逃避 the target: 1293\n",
      "when input is 会为了逃避痛 the target: 4061\n",
      "when input is 会为了逃避痛苦 the target: 1609\n",
      "when input is 会为了逃避痛苦而 the target: 3426\n",
      "when input is 会为了逃避痛苦而疯 the target: 1108\n",
      "when input is 会为了逃避痛苦而疯狂 the target: 469\n",
      "when input is 会为了逃避痛苦而疯狂， the target: 2318\n",
      "when input is 会为了逃避痛苦而疯狂，你 the target: 1346\n",
      "when input is 让 the target: 1442\n",
      "when input is 让人 the target: 1345\n",
      "when input is 让人精 the target: 2390\n",
      "when input is 让人精疲 the target: 2139\n",
      "when input is 让人精疲力 the target: 1333\n",
      "when input is 让人精疲力竭 the target: 2927\n",
      "when input is 让人精疲力竭了 the target: 469\n",
      "when input is 让人精疲力竭了， the target: 1453\n",
      "when input is 让人精疲力竭了，不 the target: 3647\n",
      "when input is 让人精疲力竭了，不是 the target: 1095\n",
      "when input is 让人精疲力竭了，不是吗 the target: 3466\n",
      "when input is 让人精疲力竭了，不是吗？ the target: 224\n",
      "when input is 苦 the target: 3198\n",
      "when input is 苦呗 the target: 3466\n",
      "when input is 苦呗？ the target: 1293\n",
      "when input is 苦呗？痛 the target: 4061\n",
      "when input is 苦呗？痛苦 the target: 186\n",
      "when input is 苦呗？痛苦并 the target: 1453\n",
      "when input is 苦呗？痛苦并不 the target: 2456\n",
      "when input is 苦呗？痛苦并不需 the target: 1711\n",
      "when input is 苦呗？痛苦并不需要 the target: 464\n",
      "when input is 苦呗？痛苦并不需要全 the target: 1877\n",
      "when input is 苦呗？痛苦并不需要全部 the target: 2184\n",
      "when input is 苦呗？痛苦并不需要全部避 the target: 353\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(9907)\n",
    "batch_size = 4 # how many independent sequences will we process in parallel?\n",
    "block_size =12 # what is the maximum context length for predictions?\n",
    "\n",
    "def get_batch(split):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    data_list = train_data if split == 'train' else val_data\n",
    "    X, Y = [], []\n",
    "    for data in data_list:\n",
    "      # avoid too short answer\n",
    "      if len(data) <= block_size:continue\n",
    "      ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "      x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "      y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "      X.append(x)\n",
    "      Y.append(y)\n",
    "    return torch.cat(X, dim=0).to('cuda'), torch.cat(Y, dim=0).to('cuda')\n",
    "\n",
    "xb, yb = get_batch('train')\n",
    "print('inputs:')\n",
    "print(xb.shape)\n",
    "print(xb)\n",
    "print('targets:')\n",
    "print(yb.shape)\n",
    "print(yb)\n",
    "\n",
    "print('----')\n",
    "\n",
    "for b in range(batch_size): # batch dimension\n",
    "    for t in range(block_size): # time dimension\n",
    "        context = xb[b, :t+1]\n",
    "        target = yb[b,t]\n",
    "        print(f\"when input is {decode(context.tolist())} the target: {target}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nql_1ER53oCf",
    "outputId": "b2edf883-de7c-4920-c81e-421f9dc4ad57"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([43200, 4179])\n",
      "tensor(8.8286, grad_fn=<NllLossBackward0>)\n",
      "闻娄侠驿亩盖灼骊技辩勤遇典愤疵轮诸彈陵跨咂鼻掖摧谋慰辍睹猿君勝庇雪奥发矫辘．记烧焖吵纪锤拽召鹊签荒朴》放扎眩箪仑幼陀嘛迫骤谜诠契易槜致踩篆制郡山揖在封深D敞特蜡此阿滚佰墩盗娄亲忑阿铃蝇掀栈狭毫螨晦踪蜡责\n",
      "[0, 2996, 1276, 1933, 469, 1954, 2308, 2513, 3571, 484, 3843, 25, 3332, 3215, 4004, 2466, 1229, 2073, 2347, 3192, 1770, 276, 265, 3511, 1362, 1293, 414, 1239, 1741, 3612, 3892, 2022, 3921, 173, 706, 3891, 314, 599, 3903, 3135, 1944, 1998, 713, 253, 1028, 899, 746, 2231, 1633, 929, 1233, 3686, 2564, 3479, 2330, 3229, 91, 3848, 2830, 2307, 2792, 2738, 2581, 2198, 3533, 578, 3216, 4000, 4053, 1140, 1569, 2547, 2911, 2503, 2579, 3535, 877, 1549, 3938, 2582, 318, 4000, 1533, 1902, 1244, 4104, 258, 3694, 2403, 2103, 3231, 4061, 1809, 2681, 2535, 2665, 1558, 3813, 1040, 4105, 1687]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "class BigramLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        logits = self.token_embedding_table(idx) # (B,T,C)\n",
    "        \n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx\n",
    "\n",
    "m = BigramLanguageModel(len(chars))\n",
    "logits, loss = m(xb, yb)\n",
    "print(logits.shape)\n",
    "print(loss)\n",
    "\n",
    "print(decode(m.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=100)[0].tolist()))\n",
    "print((m.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=100)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "eTyJ8qAaDdiF"
   },
   "outputs": [],
   "source": [
    "# create a PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in range(len(train_data)):\n",
    "    torch.save(train_data[i].to(torch.device('cpu')), str(i)+'train.pth')\n",
    "for i in range(len(val_data)):\n",
    "    torch.save(val_data[i].to(torch.device('cpu')), str(i)+ 'val.pth')\n",
    "\n",
    "# y = torch.load('0train.pth')\n",
    "# torch.equal(y, train_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import torch\n",
    "train_data = [torch.load(str(i)+'train.pth') for i in range(900)]\n",
    "val_data = [torch.load(str(i)+'train.pth') for i in range(100)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# verify\n",
    "# for i in range(900):\n",
    "#     if not torch.equal(train_data[i],train_data2[i]):\n",
    "#         print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 244
    },
    "id": "Hs4kI8YdEkQj",
    "outputId": "13eba000-5deb-4e7e-a04e-272bdb240b59"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "248.2430076599121\n",
      "2419.103317260742\n",
      "4604.466238021851\n",
      "6763.673887252808\n",
      "8915.599104642868\n",
      "8.746464729309082\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "s_t = time.time()\n",
    "batch_size = 50\n",
    "for step in range(50): # increase number of steps for good results... \n",
    "    \n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = m(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if step % 10 == 0:\n",
    "        print(time.time()-s_t)\n",
    "\n",
    "print(loss.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EcVIDWAZEtjN",
    "outputId": "0ad6f9d2-ad58-4498-a5f8-6f31407bb18b"
   },
   "outputs": [],
   "source": [
    "#print(decode(m.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=500)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "Gsxi0oljToUp"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "0.284627 M parameters\n",
      "step 0: train loss 8.5094, val loss 8.5158, time 40.53957009315491\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 1.12 GiB (GPU 0; 6.00 GiB total capacity; 3.51 GiB already allocated; 0 bytes free; 4.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 195\u001b[0m\n\u001b[0;32m    193\u001b[0m     logits, loss \u001b[38;5;241m=\u001b[39m model(xb, yb)\n\u001b[0;32m    194\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad(set_to_none\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m--> 195\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    196\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m    198\u001b[0m \u001b[38;5;66;03m# generate from the model\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    478\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    479\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    480\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    485\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    486\u001b[0m     )\n\u001b[1;32m--> 487\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    488\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    489\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\autograd\\__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    195\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    197\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    198\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    199\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 200\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    201\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    202\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 1.12 GiB (GPU 0; 6.00 GiB total capacity; 3.51 GiB already allocated; 0 bytes free; 4.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import time\n",
    "# hyperparameters\n",
    "batch_size = 8 # how many independent sequences will we process in parallel?\n",
    "block_size = 10 # what is the maximum context length for predictions?\n",
    "max_iters = 2500\n",
    "eval_interval = 100\n",
    "learning_rate = 1e-3\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)\n",
    "eval_iters = 100\n",
    "n_embd = 32\n",
    "n_head = 4\n",
    "n_layer = 1\n",
    "dropout = 0.0\n",
    "vocab_size = 4179\n",
    "# ------------\n",
    "\n",
    "torch.manual_seed(9907)\n",
    "def get_batch(split):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    data_list = train_data if split == 'train' else val_data\n",
    "    X, Y = [], []\n",
    "    for data in data_list:\n",
    "      # avoid too short answer\n",
    "      if len(data) <= block_size:continue\n",
    "      ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "      x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "      y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "      X.append(x)\n",
    "      Y.append(y)\n",
    "    return torch.cat(X, dim=0).to('cuda'), torch.cat(Y, dim=0).to('cuda')\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out\n",
    "\n",
    "class Head(nn.Module):\n",
    "    \"\"\" one head of self-attention \"\"\"\n",
    "\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B,T,C = x.shape\n",
    "        k = self.key(x)   # (B,T,C)\n",
    "        q = self.query(x) # (B,T,C)\n",
    "        # compute attention scores (\"affinities\")\n",
    "        wei = q @ k.transpose(-2,-1) * C**-0.5 # (B, T, C) @ (B, C, T) -> (B, T, T)\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
    "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
    "        wei = self.dropout(wei)\n",
    "        # perform the weighted aggregation of the values\n",
    "        v = self.value(x) # (B,T,C)\n",
    "        out = wei @ v # (B, T, T) @ (B, T, C) -> (B, T, C)\n",
    "        return out\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
    "\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(n_embd, n_embd)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out\n",
    "\n",
    "class FeedFoward(nn.Module):\n",
    "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embd, n_embd),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class Block(nn.Module):\n",
    "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        self.sa = MultiHeadAttention(n_head, head_size)\n",
    "        self.ffwd = FeedFoward(n_embd)\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.sa(self.ln1(x))\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "# super simple bigram model\n",
    "class BigramLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
    "        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
    "        x = tok_emb + pos_emb # (B,T,C)\n",
    "        x = self.blocks(x) # (B,T,C)\n",
    "        x = self.ln_f(x) # (B,T,C)\n",
    "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # crop idx to the last block_size tokens\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx_cond)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx\n",
    "\n",
    "model = BigramLanguageModel()\n",
    "m = model.to(device)\n",
    "# print the number of parameters in the model\n",
    "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
    "\n",
    "# create a PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "#love_context = torch.tensor(encode('我爱你'), dtype=torch.long).to(device)\n",
    "st = time.time()\n",
    "\n",
    "for iter in range(max_iters):\n",
    "\n",
    "    # every once in a while evaluate the loss on train and val sets\n",
    "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}, time {time.time()-st}\")\n",
    "        #print(decode(m.generate(love_context, max_new_tokens=50)[0].tolist()))\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "# generate from the model\n",
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "print(decode(m.generate(context, max_new_tokens=2000)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "torch.cuda.max_memory_allocated('cuda')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XinV8nmAnmKN"
   },
   "source": [
    "## The mathematical trick in self-attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tukiH-NbRBhA",
    "outputId": "d981f6d4-ac08-4ec2-8284-82f5fa1e0815"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a=\n",
      "tensor([[1.0000, 0.0000, 0.0000],\n",
      "        [0.5000, 0.5000, 0.0000],\n",
      "        [0.3333, 0.3333, 0.3333]])\n",
      "--\n",
      "b=\n",
      "tensor([[2., 7.],\n",
      "        [6., 4.],\n",
      "        [6., 5.]])\n",
      "--\n",
      "c=\n",
      "tensor([[2.0000, 7.0000],\n",
      "        [4.0000, 5.5000],\n",
      "        [4.6667, 5.3333]])\n"
     ]
    }
   ],
   "source": [
    "# toy example illustrating how matrix multiplication can be used for a \"weighted aggregation\"\n",
    "torch.manual_seed(42)\n",
    "a = torch.tril(torch.ones(3, 3))\n",
    "a = a / torch.sum(a, 1, keepdim=True)\n",
    "b = torch.randint(0,10,(3,2)).float()\n",
    "c = a @ b\n",
    "print('a=')\n",
    "print(a)\n",
    "print('--')\n",
    "print('b=')\n",
    "print(b)\n",
    "print('--')\n",
    "print('c=')\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Hs_E24uRE8kr",
    "outputId": "8bf3ff5f-565e-48b8-de8e-7272706c8e12"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 2])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# consider the following toy example:\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "B,T,C = 4,8,2 # batch, time, channels\n",
    "x = torch.randn(B,T,C)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "86NuXX0fn7ps"
   },
   "outputs": [],
   "source": [
    "# We want x[b,t] = mean_{i<=t} x[b,i]\n",
    "xbow = torch.zeros((B,T,C))\n",
    "for b in range(B):\n",
    "    for t in range(T):\n",
    "        xprev = x[b,:t+1] # (t,C)\n",
    "        xbow[b,t] = torch.mean(xprev, 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yhdOAd6-wXkZ",
    "outputId": "eaf6ab61-dff1-4bb7-e623-47f692bad5f9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# version 2: using matrix multiply for a weighted aggregation\n",
    "wei = torch.tril(torch.ones(T, T))\n",
    "wei = wei / wei.sum(1, keepdim=True)\n",
    "xbow2 = wei @ x # (B, T, T) @ (B, T, C) ----> (B, T, C)\n",
    "torch.allclose(xbow, xbow2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wOURrfG-ysoL",
    "outputId": "080b500d-8110-4602-fcef-7d6f2ebfc6bc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# version 3: use Softmax\n",
    "tril = torch.tril(torch.ones(T, T))\n",
    "wei = torch.zeros((T,T))\n",
    "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
    "wei = F.softmax(wei, dim=-1)\n",
    "xbow3 = wei @ x\n",
    "torch.allclose(xbow, xbow3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EDarxEWIRMKq",
    "outputId": "07b587dd-a91c-4bb0-d7f1-e247cd5dacb5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 16])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# version 4: self-attention!\n",
    "torch.manual_seed(1337)\n",
    "B,T,C = 4,8,32 # batch, time, channels\n",
    "x = torch.randn(B,T,C)\n",
    "\n",
    "# let's see a single Head perform self-attention\n",
    "head_size = 16\n",
    "key = nn.Linear(C, head_size, bias=False)\n",
    "query = nn.Linear(C, head_size, bias=False)\n",
    "value = nn.Linear(C, head_size, bias=False)\n",
    "k = key(x)   # (B, T, 16)\n",
    "q = query(x) # (B, T, 16)\n",
    "wei =  q @ k.transpose(-2, -1) # (B, T, 16) @ (B, 16, T) ---> (B, T, T)\n",
    "\n",
    "tril = torch.tril(torch.ones(T, T))\n",
    "#wei = torch.zeros((T,T))\n",
    "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
    "wei = F.softmax(wei, dim=-1)\n",
    "\n",
    "v = value(x)\n",
    "out = wei @ v\n",
    "#out = wei @ x\n",
    "\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vT1hdtzXCjgL",
    "outputId": "6d2c569b-7922-451f-9934-0fc564678d17"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.1574, 0.8426, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2088, 0.1646, 0.6266, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.5792, 0.1187, 0.1889, 0.1131, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.0294, 0.1052, 0.0469, 0.0276, 0.7909, 0.0000, 0.0000, 0.0000],\n",
       "        [0.0176, 0.2689, 0.0215, 0.0089, 0.6812, 0.0019, 0.0000, 0.0000],\n",
       "        [0.1691, 0.4066, 0.0438, 0.0416, 0.1048, 0.2012, 0.0329, 0.0000],\n",
       "        [0.0210, 0.0843, 0.0555, 0.2297, 0.0573, 0.0709, 0.2423, 0.2391]],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wei[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M5CvobiQ0pLr"
   },
   "source": [
    "Notes:\n",
    "- Attention is a **communication mechanism**. Can be seen as nodes in a directed graph looking at each other and aggregating information with a weighted sum from all nodes that point to them, with data-dependent weights.\n",
    "- There is no notion of space. Attention simply acts over a set of vectors. This is why we need to positionally encode tokens.\n",
    "- Each example across batch dimension is of course processed completely independently and never \"talk\" to each other\n",
    "- In an \"encoder\" attention block just delete the single line that does masking with `tril`, allowing all tokens to communicate. This block here is called a \"decoder\" attention block because it has triangular masking, and is usually used in autoregressive settings, like language modeling.\n",
    "- \"self-attention\" just means that the keys and values are produced from the same source as queries. In \"cross-attention\", the queries still get produced from x, but the keys and values come from some other, external source (e.g. an encoder module)\n",
    "- \"Scaled\" attention additional divides `wei` by 1/sqrt(head_size). This makes it so when input Q,K are unit variance, wei will be unit variance too and Softmax will stay diffuse and not saturate too much. Illustration below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4SNbLq5z3oBw"
   },
   "outputs": [],
   "source": [
    "k = torch.randn(B,T,head_size)\n",
    "q = torch.randn(B,T,head_size)\n",
    "wei = q @ k.transpose(-2, -1) * head_size**-0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Nl6I9n9IRTSo",
    "outputId": "0c5b9cd0-af8a-4564-fbad-41d844e54822"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.0449)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k.var()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "T1tQx7oeRvtc",
    "outputId": "3541ca1a-7447-4ef7-835e-81824aebc1b5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.0700)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q.var()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MLb_odHU3iKM",
    "outputId": "a687a222-5a2c-4cdb-c1bf-17cd05b45b69"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.0918)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wei.var()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JB82yzt44REI",
    "outputId": "f07da2f1-10bb-4a7a-bcaa-578587977d00"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.1925, 0.1426, 0.2351, 0.1426, 0.2872])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.softmax(torch.tensor([0.1, -0.2, 0.3, -0.2, 0.5]), dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Mpt8569BB9_f",
    "outputId": "5d8b910a-6192-44ba-ebb2-497d88e0b629"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0326, 0.0030, 0.1615, 0.0030, 0.8000])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.softmax(torch.tensor([0.1, -0.2, 0.3, -0.2, 0.5])*8, dim=-1) # gets too peaky, converges to one-hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2Num7sX9CKOH",
    "outputId": "929ceb78-a639-41d6-aac7-12997b5c93f0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 100])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class LayerNorm1d: # (used to be BatchNorm1d)\n",
    "  \n",
    "  def __init__(self, dim, eps=1e-5, momentum=0.1):\n",
    "    self.eps = eps\n",
    "    self.gamma = torch.ones(dim)\n",
    "    self.beta = torch.zeros(dim)\n",
    "  \n",
    "  def __call__(self, x):\n",
    "    # calculate the forward pass\n",
    "    xmean = x.mean(1, keepdim=True) # batch mean\n",
    "    xvar = x.var(1, keepdim=True) # batch variance\n",
    "    xhat = (x - xmean) / torch.sqrt(xvar + self.eps) # normalize to unit variance\n",
    "    self.out = self.gamma * xhat + self.beta\n",
    "    return self.out\n",
    "  \n",
    "  def parameters(self):\n",
    "    return [self.gamma, self.beta]\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "module = LayerNorm1d(100)\n",
    "x = torch.randn(32, 100) # batch size 32 of 100-dimensional vectors\n",
    "x = module(x)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "633T2cmnW1uk",
    "outputId": "7720fa58-0478-4e8a-86a7-502d4cce9443"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.1469), tensor(0.8803))"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[:,0].mean(), x[:,0].std() # mean,std of one feature across all batch inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LN9cK9BoXCYb",
    "outputId": "6368ece0-600e-417d-8a91-7c1e5d750ba8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(-9.5367e-09), tensor(1.0000))"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[0,:].mean(), x[0,:].std() # mean,std of a single input from the batch, of its features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dRJH6wM_XFfU"
   },
   "outputs": [],
   "source": [
    "# French to English translation example:\n",
    "\n",
    "# <--------- ENCODE ------------------><--------------- DECODE ----------------->\n",
    "# les réseaux de neurones sont géniaux! <START> neural networks are awesome!<END>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZcvKeBXoZFOY"
   },
   "source": [
    "### Full finished code, for reference\n",
    "\n",
    "You may want to refer directly to the git repo instead though."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hoelkOrFY8bN",
    "outputId": "961304cd-e379-40d4-dd56-8de0b91d2861"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.209729 M parameters\n",
      "step 0: train loss 4.4116, val loss 4.4022\n",
      "step 100: train loss 2.6568, val loss 2.6670\n",
      "step 200: train loss 2.5090, val loss 2.5058\n",
      "step 300: train loss 2.4198, val loss 2.4340\n",
      "step 400: train loss 2.3503, val loss 2.3567\n",
      "step 500: train loss 2.2970, val loss 2.3136\n",
      "step 600: train loss 2.2410, val loss 2.2506\n",
      "step 700: train loss 2.2062, val loss 2.2198\n",
      "step 800: train loss 2.1638, val loss 2.1871\n",
      "step 900: train loss 2.1232, val loss 2.1494\n",
      "step 1000: train loss 2.1020, val loss 2.1293\n",
      "step 1100: train loss 2.0704, val loss 2.1196\n",
      "step 1200: train loss 2.0382, val loss 2.0798\n",
      "step 1300: train loss 2.0249, val loss 2.0640\n",
      "step 1400: train loss 1.9922, val loss 2.0354\n",
      "step 1500: train loss 1.9707, val loss 2.0308\n",
      "step 1600: train loss 1.9614, val loss 2.0474\n",
      "step 1700: train loss 1.9393, val loss 2.0130\n",
      "step 1800: train loss 1.9070, val loss 1.9943\n",
      "step 1900: train loss 1.9057, val loss 1.9871\n",
      "step 2000: train loss 1.8834, val loss 1.9954\n",
      "step 2100: train loss 1.8719, val loss 1.9758\n",
      "step 2200: train loss 1.8582, val loss 1.9623\n",
      "step 2300: train loss 1.8546, val loss 1.9517\n",
      "step 2400: train loss 1.8410, val loss 1.9476\n",
      "step 2500: train loss 1.8167, val loss 1.9455\n",
      "step 2600: train loss 1.8263, val loss 1.9401\n",
      "step 2700: train loss 1.8108, val loss 1.9340\n",
      "step 2800: train loss 1.8040, val loss 1.9247\n",
      "step 2900: train loss 1.8044, val loss 1.9304\n",
      "step 3000: train loss 1.7963, val loss 1.9242\n",
      "step 3100: train loss 1.7687, val loss 1.9147\n",
      "step 3200: train loss 1.7547, val loss 1.9102\n",
      "step 3300: train loss 1.7557, val loss 1.9037\n",
      "step 3400: train loss 1.7547, val loss 1.8946\n",
      "step 3500: train loss 1.7385, val loss 1.8968\n",
      "step 3600: train loss 1.7260, val loss 1.8914\n",
      "step 3700: train loss 1.7257, val loss 1.8808\n",
      "step 3800: train loss 1.7204, val loss 1.8919\n",
      "step 3900: train loss 1.7215, val loss 1.8788\n",
      "step 4000: train loss 1.7146, val loss 1.8639\n",
      "step 4100: train loss 1.7095, val loss 1.8724\n",
      "step 4200: train loss 1.7079, val loss 1.8707\n",
      "step 4300: train loss 1.7035, val loss 1.8502\n",
      "step 4400: train loss 1.7043, val loss 1.8693\n",
      "step 4500: train loss 1.6914, val loss 1.8522\n",
      "step 4600: train loss 1.6853, val loss 1.8357\n",
      "step 4700: train loss 1.6862, val loss 1.8483\n",
      "step 4800: train loss 1.6671, val loss 1.8434\n",
      "step 4900: train loss 1.6736, val loss 1.8415\n",
      "step 4999: train loss 1.6635, val loss 1.8226\n",
      "\n",
      "FlY BOLINGLO:\n",
      "Them thrumply towiter arts the\n",
      "muscue rike begatt the sea it\n",
      "What satell in rowers that some than othis Marrity.\n",
      "\n",
      "LUCENTVO:\n",
      "But userman these that, where can is not diesty rege;\n",
      "What and see to not. But's eyes. What?\n",
      "\n",
      "JOHN MARGARET:\n",
      "Than up I wark, what out, I ever of and love,\n",
      "one these do sponce, vois I me;\n",
      "But my pray sape to ries all to the not erralied in may.\n",
      "\n",
      "BENVOLIO:\n",
      "To spits as stold's bewear I would and say mesby all\n",
      "on sworn make he anough\n",
      "As cousins the solle, whose be my conforeful may lie them yet\n",
      "nobe allimely untraled to be thre I say be,\n",
      "Notham a brotes theme an make come,\n",
      "And that his reach to the duke ento\n",
      "the grmeants bell! and now there king-liff-or grief?\n",
      "\n",
      "GLOUCESTER:\n",
      "All the bettle dreene, for To his like thou thron!\n",
      "\n",
      "MENENIUS:\n",
      "Then, if I knom her all.\n",
      "My lord, but terruly friend\n",
      "Rish of the ploceiness and wilt tends sure?\n",
      "Is you knows a fasir wead\n",
      "That with him my spaut,\n",
      "I shall not tas where's not, becomity; my coulds sting,\n",
      "then the wit be dong to tyget our hereefore,\n",
      "Who strop me, mend here, if agains, bitten, thy lack.\n",
      "The but these it were is tus. For the her skeep the fasting. joy tweet Bumner:-\n",
      "How the enclady: It you and how,\n",
      "I am in him, And ladderle:\n",
      "Their hand whose wife, it my hithre,\n",
      "Roman and where sposs gives'd you.\n",
      "\n",
      "TROMIOLANUS:\n",
      "But livants you great, I shom mistrot come, for to she to lot\n",
      "for smy to men ventry mehus. Gazise;\n",
      "Full't were some the cause, and stouch set,\n",
      "Or promises, which a kingsasted to your gove them; and sterrer,\n",
      "And that wae love him.\n",
      "\n",
      "BRUTUS:\n",
      "You shape with these sweet.\n",
      "\n",
      "CORTENGONO:\n",
      "Lo, where 'twon elmes, 'morth young agres;\n",
      "Sir, azavoust to striel accurded we missery sets crave.\n",
      "\n",
      "ANGOLUM:\n",
      "For is Henry to have gleise the dreason\n",
      "That I ant shorfold wefth their servy in enscy.\n",
      "\n",
      "ISABELLA:\n",
      "O, I better you eyse such formfetrews.\n",
      "\n",
      "BUCKINGHARENT:\n",
      "Qead my lightle this righanneds flase them\n",
      "Wam which an take was our some pleasurs,\n",
      "Lovisoname to me, then fult me?--have it?\n",
      "\n",
      "HENRY BOLINGBROY:\n",
      "That wha\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "# hyperparameters\n",
    "batch_size = 16 # how many independent sequences will we process in parallel?\n",
    "block_size = 32 # what is the maximum context length for predictions?\n",
    "max_iters = 5000\n",
    "eval_interval = 100\n",
    "learning_rate = 1e-3\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "eval_iters = 200\n",
    "n_embd = 64\n",
    "n_head = 4\n",
    "n_layer = 4\n",
    "dropout = 0.0\n",
    "# ------------\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "# wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
    "with open('input.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "# here are all the unique characters that occur in this text\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "# create a mapping from characters to integers\n",
    "stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "itos = { i:ch for i,ch in enumerate(chars) }\n",
    "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
    "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
    "\n",
    "# Train and test splits\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "\n",
    "# data loading\n",
    "def get_batch(split):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out\n",
    "\n",
    "class Head(nn.Module):\n",
    "    \"\"\" one head of self-attention \"\"\"\n",
    "\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B,T,C = x.shape\n",
    "        k = self.key(x)   # (B,T,C)\n",
    "        q = self.query(x) # (B,T,C)\n",
    "        # compute attention scores (\"affinities\")\n",
    "        wei = q @ k.transpose(-2,-1) * C**-0.5 # (B, T, C) @ (B, C, T) -> (B, T, T)\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
    "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
    "        wei = self.dropout(wei)\n",
    "        # perform the weighted aggregation of the values\n",
    "        v = self.value(x) # (B,T,C)\n",
    "        out = wei @ v # (B, T, T) @ (B, T, C) -> (B, T, C)\n",
    "        return out\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
    "\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(n_embd, n_embd)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out\n",
    "\n",
    "class FeedFoward(nn.Module):\n",
    "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embd, n_embd),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class Block(nn.Module):\n",
    "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        self.sa = MultiHeadAttention(n_head, head_size)\n",
    "        self.ffwd = FeedFoward(n_embd)\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.sa(self.ln1(x))\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "# super simple bigram model\n",
    "class BigramLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
    "        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
    "        x = tok_emb + pos_emb # (B,T,C)\n",
    "        x = self.blocks(x) # (B,T,C)\n",
    "        x = self.ln_f(x) # (B,T,C)\n",
    "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # crop idx to the last block_size tokens\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx_cond)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx\n",
    "\n",
    "model = BigramLanguageModel()\n",
    "m = model.to(device)\n",
    "# print the number of parameters in the model\n",
    "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
    "\n",
    "# create a PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for iter in range(max_iters):\n",
    "\n",
    "    # every once in a while evaluate the loss on train and val sets\n",
    "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "# generate from the model\n",
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "print(decode(m.generate(context, max_new_tokens=2000)[0].tolist()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fjjvMifYZf7x"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
